{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"hw3_genesT.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data.iloc[:, 0:1000], data[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in search of perfect separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeanclus = KMeans(n_clusters=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push cluster and actual class into dataframe to see if cluster separated perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Cluster Class\n",
       "0         1     H\n",
       "1         1     H\n",
       "2         1     H\n",
       "3         1     H\n",
       "4         1     H\n",
       "5         1     H\n",
       "6         1     H\n",
       "7         1     H\n",
       "8         1     H\n",
       "9         1     H\n",
       "10        1     H\n",
       "11        1     H\n",
       "12        1     H\n",
       "13        1     H\n",
       "14        1     H\n",
       "15        1     H\n",
       "16        1     H\n",
       "17        1     H\n",
       "18        1     H\n",
       "19        1     H\n",
       "20        0     U\n",
       "21        0     U\n",
       "22        0     U\n",
       "23        0     U\n",
       "24        0     U\n",
       "25        0     U\n",
       "26        0     U\n",
       "27        0     U\n",
       "28        0     U\n",
       "29        0     U\n",
       "30        0     U\n",
       "31        0     U\n",
       "32        0     U\n",
       "33        0     U\n",
       "34        0     U\n",
       "35        0     U\n",
       "36        0     U\n",
       "37        0     U\n",
       "38        0     U\n",
       "39        0     U"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_results = pd.DataFrame()\n",
    "cluster_results[\"Cluster\"], cluster_results[\"Class\"] = kmeanclus.labels_, y \n",
    "cluster_results\n",
    "# indeed, they cluster perfectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection to establish feature matrix, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_newl1 = model.transform(X)\n",
    "X_newl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 424)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = data.iloc[:, 0:1000], data[1000]\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l2\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_newl2 = model.transform(X)\n",
    "X_newl2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results of l1 & l2 penalty for feature selection\n",
    "\n",
    "When applying the L1 penalty for feature selection we are in search of a sparse solution that is able to map X to y. The L1 - regularized loss function is given by  F(x)=f(x)+λ||x||1, a non-smooth function. The L1 penalty will remove one of two variables that are highly correlated, which leads to sparsity. Alternatively the L2 penalty will keep both variables instead constraining the of these variables coefficients, thus reducing the variance in our predictions. \n",
    "\n",
    "The variables are removed due to the point at which the tangent line touches the L1 and L2 norm. In a two-dimensional L1 space we see that at this point the x2 or x1 must be zero; in the L2 space there is no such contraint. \n",
    "\n",
    "In this case the L1 penalty used to select features drives us to a one-dimensional predictor space. The L2 penalty reduces the dimensions but not nearly as much.\n",
    "\n",
    "In most cases, the L1 penalty would reduce the dimensions and eliminate the risk of overfitting. While the L2 penalty would reduce risk of overfitting without a loss in prediction power by not eliminating features of our predictor space. That is to say that the L1 penalty is likely to hurt predictive power due to the loss of features, in this case a drastic loss.\n",
    "\n",
    "In this case the L1 penalty reduces our feature space to one-dimension. This feature space is intriguing because it seems that a one-dimensional space is more likely to be prone to overfitting, the direct problem trying to be solved by the L1 penalty. The L2 penalty just about cut our feature space in half thus reducing variance in our predictions while not shrinking our predictor space nearly as much.\n",
    "\n",
    "I would like to use the L1 penalty and the L2 penalty in a pipeline with a random forest model and estimate prediction error produced by each method. I will also build a random forest that does not have its features preprocessed, which serves as a control.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build model using L2 for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "        prefit=False, thres...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l2\", dual=False))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building model using L1 for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_newl1 = model.transform(X)\n",
    "X_newl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trainl1, X_testl1, y_trainl1, y_testl1 = train_test_split(X_newl1, y, test_size=0.5, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-dimensional feature space\n",
    "X_trainl1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_modl1=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_modl1.fit(X_trainl1, y_trainl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_modl1.score(X_testl1, y_testl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0.9,  0.1],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0.2,  0.8],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 0. ,  1. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 1. ,  0. ],\n",
       "       [ 0.5,  0.5],\n",
       "       [ 0.2,  0.8],\n",
       "       [ 1. ,  0. ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_modl1.predict_proba(X_testl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Build model using no feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mod_no_preprocess=RandomForestClassifier()\n",
    "rf_mod_no_preprocess.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mod_no_preprocess.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of random forest before moving on to linear modeling with a logistic regression\n",
    "\n",
    "The random forest preprocessed with L2 and a random forest without preprocessing perform similar in predicting the classes. While the L1 penalty is performs the worst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going linear with a Logistic Regression - w/ L1, L2, and elastic net\n",
    "\n",
    "The next thing I will do is build a logistic regression using the L1 penalty, L2 penalty, and Elastic Net. Elastic Net uses the optimal balance between the L1 and L2 regularization terms while only adding the computational cost of optimizing another hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a logisitic regression with L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logl1= LogisticRegression(penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84999999999999998"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a logistic regression with L2 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logl2= LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69999999999999996"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logl2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building logistic regression with elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_en = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_en.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_en.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap thus far\n",
    "\n",
    "Logistic with L1 penalty out performs both elastic net and L2 penalty applied with logistic regression. But when comparing the results of this linear classifier with our non-linear random forest we get different results. The random forest performs better overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H', 'U', 'U', 'U', 'U', 'H', 'U', 'H', 'H', 'U', 'H', 'H', 'U',\n",
       "       'U', 'H', 'H', 'H', 'H', 'U', 'H'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the predictions from the best non-linear model - random forest with L2\n",
    "y_hat_rfl2 = clf.predict(X_test)\n",
    "y_hat_rfl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U', 'U', 'U', 'U', 'U', 'U', 'U', 'H', 'H', 'U', 'H', 'U', 'U',\n",
       "       'U', 'H', 'H', 'U', 'U', 'U', 'H'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the predictions from the best linear model - logistic regression with L1 penalty\n",
    "y_hat_lgrl1 = logl1.predict(X_test)\n",
    "y_hat_lgrl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "-Random Forest w/ L2 feature selection classification report-\n",
      "-------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Healthy       0.82      1.00      0.90         9\n",
      "  Unhealthy       1.00      0.82      0.90        11\n",
      "\n",
      "avg / total       0.92      0.90      0.90        20\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------------------\n",
      "------Logistic Regression w/ Lasso classification report-----\n",
      "-------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Healthy       1.00      0.67      0.80         9\n",
      "  Unhealthy       0.79      1.00      0.88        11\n",
      "\n",
      "avg / total       0.88      0.85      0.84        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['Healthy', 'Unhealthy']\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"-Random Forest w/ L2 feature selection classification report-\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(classification_report(y_test, y_hat_rfl2, target_names=target_names))\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"------Logistic Regression w/ Lasso classification report-----\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(classification_report(y_test, y_hat_lgrl1, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-point\n",
    "\n",
    "The random forest classifier with L2 used for features selection gives better predictions than the logistic regression with a lasso (L1) penalty. \n",
    "\n",
    "Let's move foward with the random forest and see if other non-linear techniques can boost our ability to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building on our results with AdaBoost classifier\n",
    "\n",
    "The AdaBoost classifier iteratively builds what are know as weak learners. A weak learner is trained on a subset of the features space and is constrained with how deep it may grow, i.e., we limit the number of splits in can make. The first weak learner classifies each instance of our training set recording the instances which it got right and wrong. The instances that are classified as wrong are given weights that allow the next weak learner to \"focus\" on correctly classifying the instances the weak learner that came before got wrong. The process continues in this interative way where the new weak learner will not be built until after the errors of the weak learner before it are noted and given weights. The weak learners are then used to predict the training set and the majority vote for class 0 or 1 are used to classify the instances in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=None, n_estimators=50, \n",
    "                   learning_rate=1.0, algorithm='SAMME.R', \n",
    "                   random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline: L2 feature selection and AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "        prefit=False, threshold=None)), ('classification', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaL2 = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l2\", dual=False))),\n",
    "  ('classification', AdaBoostClassifier())\n",
    "])\n",
    "adaL2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaL2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat_ada = ada.predict(X_test)\n",
    "y_hat_adaL2 = adaL2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "-----------------AdaBoost classification report--------------\n",
      "-------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Healthy       0.78      0.78      0.78         9\n",
      "  Unhealthy       0.82      0.82      0.82        11\n",
      "\n",
      "avg / total       0.80      0.80      0.80        20\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------------------\n",
      "Pipeline:  L2 feature select & AdaBoost classification report\n",
      "-------------------------------------------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Healthy       0.73      0.89      0.80         9\n",
      "  Unhealthy       0.89      0.73      0.80        11\n",
      "\n",
      "avg / total       0.82      0.80      0.80        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Healthy', 'Unhealthy']\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"-----------------AdaBoost classification report--------------\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(classification_report(y_test, y_hat_ada, target_names=target_names))\n",
    "print(\"\\n\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"Pipeline:  L2 feature select & AdaBoost classification report\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(classification_report(y_test, y_hat_adaL2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's provide AdaBoost different features\n",
    "\n",
    "The AdaBoost algorithm can be prone to outliers and noisy data due to its processes at each iteration. If outliers and noise is found in our data the weak learners will adjust to fit such noise at the cost of missing our signal. Once the model is used to predict on test data it will have trouble because it has fit noise and will subsequently fail to map our features to the true binomial distribution of the response variable. As we saw before L2 feature selection leaves us with large feature space (p > 400) which could be contributing to the relative error and instability of the predictions.\n",
    "\n",
    "In order to reduce our feature space let's use the random forest from earlier to identify variables based on their importance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct random forest without pipeline in order to extract feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 424)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc = LinearSVC(C=0.01, penalty=\"l2\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_newL2 = model.transform(X)\n",
    "X_newL2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trainL2, X_testL2, y_trainL2, y_testL2 = train_test_split(X_newL2, y, test_size=0.5, random_state=57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_modL2=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_modL2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp_feat = rf_modL2.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of most important variables: [506, 507, 508, 514, 524, 547, 548, 564, 578, 588, 589]\n"
     ]
    }
   ],
   "source": [
    "index_imp_feats = []\n",
    "for i in range(0,len(imp_feat)):\n",
    "    if imp_feat[i] != 0.0:\n",
    "        index_imp_feats.append(i)\n",
    "print(\"Columns of most important variables:\", index_imp_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store subset of X_train and X_test\n",
    "# only made up of most important features from random forest\n",
    "X_imp_feats_train = X_train.ix[:, [x for x in index_imp_feats]]\n",
    "X_imp_feats_test = X_test.ix[:, [x for x in index_imp_feats]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building AdaBoost model with most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_imp_feats = AdaBoostClassifier(base_estimator=None, n_estimators=300, \n",
    "                   learning_rate=1.0, algorithm='SAMME.R', \n",
    "                   random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=300, random_state=80)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_imp_feats.fit(X_imp_feats_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_imp_feats.score(X_imp_feats_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building random forest with only most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_imp_feats = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_imp_feats.fit(X_imp_feats_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_imp_feats.score(X_imp_feats_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building random forest & L2 pipeline starting with only important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "        prefit=False, thres...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_imp_feats_pipe = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(C=0.01, penalty=\"l2\", dual=False))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "rf_imp_feats_pipe.fit(X_imp_feats_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_imp_feats_pipe.score(X_imp_feats_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap - to this point\n",
    "\n",
    "The results are straightforward we get an improvment in the AdaBoost algorithm and the random forest when we use a subset of the most important 15 features. But this is not good enough. Let's take the *best of the best* features and see how our predictions perform with AdaBoost and random forest. Here I will use the top 5 important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_top_feats_train = X_train.ix[:, [x for x in index_imp_feats[0:5]]]\n",
    "X_top_feats_test = X_test.ix[:, [x for x in index_imp_feats[0:5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>514</th>\n",
       "      <th>524</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.286128</td>\n",
       "      <td>1.405476</td>\n",
       "      <td>-1.477960</td>\n",
       "      <td>0.520690</td>\n",
       "      <td>0.345878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.659982</td>\n",
       "      <td>3.364177</td>\n",
       "      <td>2.793809</td>\n",
       "      <td>2.149940</td>\n",
       "      <td>1.507902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.873723</td>\n",
       "      <td>-1.537878</td>\n",
       "      <td>0.387909</td>\n",
       "      <td>2.098149</td>\n",
       "      <td>0.062585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         506       507       508       514       524\n",
       "18 -1.286128  1.405476 -1.477960  0.520690  0.345878\n",
       "37  2.659982  3.364177  2.793809  2.149940  1.507902\n",
       "4  -0.873723 -1.537878  0.387909  2.098149  0.062585"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first three rows of our new training set\n",
    "# we see top five imp features columns from original data\n",
    "X_top_feats_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_top_feats = AdaBoostClassifier(base_estimator=None, n_estimators=50, \n",
    "                   learning_rate=1.0, algorithm='SAMME.R', \n",
    "                   random_state=82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=82)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_top_feats.fit(X_top_feats_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_top_feats.score(X_top_feats_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_top_feats = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_top_feats.fit(X_top_feats_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94999999999999996"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_top_feats.score(X_top_feats_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifying cancer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancdf = pd.read_csv(\"cancer_data_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Patients', 'Class', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6',\n",
       "       'X7',\n",
       "       ...\n",
       "       'X6821', 'X6822', 'X6823', 'X6824', 'X6825', 'X6826', 'X6827', 'X6828',\n",
       "       'X6829', 'X6830'],\n",
       "      dtype='object', length=6833)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_canc, y_canc = cancdf.ix[:, 3:], cancdf[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trainc, X_testc, y_trainc, y_testc = train_test_split(X_canc, y_canc, test_size=0.5, random_state=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:515: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "log_2 = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "log_2.fit(X_trainc, y_trainc)\n",
    "f1score = log_2.score(X_testc, y_testc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Using logistic regression with elastic net regularization----\n",
      "f1-score: 0.666666666667\n",
      "-----------------------------------------------------------\n",
      "\n",
      "\n",
      "Cancer type, most important feature (by column number)\n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "Breast: [3863 3827 6687  162 5889 4447 3826 2748 5877 6603 5890 5891  515 6716  280]\n",
      "Renal: [4041 5891 3690  805 6409 5868 6645 3559 6688 3542 3862 3863 3247 6687 3437]\n",
      "CNS: [3923 5892  232 3559 5384 5382 3862 4246 2008  285 5456 3524 5383 5455 4247]\n",
      "Colon: [6687 3828 3826 3225 1048 3690 6688 3864 6457 3827 3862 3863 3247 3437 4100]\n",
      "Ovarian: [5528 6688 3825 5868 4573 4100 3863 5983 3862 3826 3827 3690  805 6687 3867]\n",
      "Melanoma: [6277 5643 6355  514 5868 3559 5837 5893 4279 6687  243 6462 5533 4105 3542]\n",
      "NSCLC: [ 805 3501 6688  515 5650  516 5585  514 5790 5936 5983 6017 3437 6687  132]\n",
      "Leukemia: [5528 3691  200 3827 3826 6017 5527 5867 4573 3690 6645 3542 3437 5868  195]\n"
     ]
    }
   ],
   "source": [
    "print(\"----Using logistic regression with elastic net regularization----\")\n",
    "print(\"f1-score: \" + str(f1score))\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "print(\"Cancer type, \" + \"most important feature (by column number)\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "for i in range(0, log_2.coef_.shape[0]):\n",
    "    top15_indices = np.argsort(log_2.coef_[i])[-15:]\n",
    "    if str(y_trainc['Class'].unique()[i]) == \"NSCLC\":\n",
    "        print(str(y_trainc['Class'].unique()[i]).upper() + \": \" + str(top15_indices))\n",
    "    elif str(y_trainc['Class'].unique()[i]) == \"CNS\":\n",
    "        print(str(y_trainc['Class'].unique()[i]).upper() + \": \" + str(top15_indices))\n",
    "    else:\n",
    "        print(str(y_trainc['Class'].unique()[i]).title() + \": \" + str(top15_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_feats = {}\n",
    "for i in range(0, log_2.coef_.shape[0]):\n",
    "    top15_indices = np.argsort(log_2.coef_[i])[-15:]\n",
    "    for feat in top15_indices:\n",
    "        if feat not in important_feats:\n",
    "            important_feats[feat] = 0\n",
    "        important_feats[feat] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Column  Count\n",
       "132     132      1\n",
       "162     162      1\n",
       "195     195      1\n",
       "200     200      1\n",
       "232     232      1\n",
       "243     243      1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df = pd.DataFrame([important_feats])\n",
    "dict_df = dict_df.T\n",
    "dict_df[\"Column\"] = dict_df.index\n",
    "dict_df[\"Count\"] = dict_df[0]\n",
    "del dict_df[0]\n",
    "dict_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 columns by number of cancers, for which, they are a top 15 feature as determined by the coefficient in logistic regression.\n",
    "\n",
    "For example, the features in columns 6687 and 3437 are in the top 15 features for 5 of the 9 cancers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6687,    6],\n",
       "       [3690,    4],\n",
       "       [3437,    4],\n",
       "       [3862,    4],\n",
       "       [5868,    4],\n",
       "       [3827,    4],\n",
       "       [3826,    4],\n",
       "       [3863,    4],\n",
       "       [6688,    4],\n",
       "       [ 805,    3]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_df.sort_values(\"Count\", ascending=False).values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
