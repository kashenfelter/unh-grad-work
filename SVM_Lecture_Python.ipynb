{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pylab import rcParams\n",
    "%pylab inline\n",
    "plt.rcParams['figure.figsize'] = (8.5, 5.5) ## set default plot size for notebook\n",
    "plt.style.use('ggplot') ## set ggplot style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machine intuition with examples in Python\n",
    "\n",
    "#### Starting with Logistic Regression\n",
    "\n",
    "##### We use the sigmoid function:\n",
    "$h(x) = {1 \\over 1 + e^{-\\beta^Tx}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+np.exp(-item)))\n",
    "    return a\n",
    "    \n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "#plt.axhline(0.95, color = 'red')\n",
    "plt.axvline(4.6, color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $z$ is equal to 4.6 it corresponds to probability of 0.99 once passed through the sigmoid function\n",
    "\n",
    "$g(4.6) = 0.99$\n",
    "\n",
    "where,\n",
    "- $z$ = $\\beta^Tx$\n",
    "- $h(x) = 0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### our 'prediction' of y is defined by\n",
    "$h(x) = g(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### In the context of a binary classification problem our prediction is the output of pasing $z$ through the sigmoid function. \n",
    "remember $z = \\beta^Tx$ \n",
    "\n",
    "#### If $y = 1$, we would like our predicted probability $h_\\beta \\approx 1$,  therefore, $\\beta^Tx \\gg 0$. Note that, $h_\\beta \\approx 1$ corresponds to $z > 4.6$.\n",
    "\n",
    "#### If $y = 0$, we would like our predicted probability $h_\\beta \\approx 0$, therefore, $\\beta^Tx \\ll 0$. Note that, $h_\\beta \\approx 1$ corresponds to $z < -4.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively \n",
    "\n",
    "#### We have our cost function for ***each*** training example in Logisitic Regression\n",
    "$-\\Big(y \\ log \\ h_\\beta(x) + (1-y) log(1-h_\\beta(x))\\Big)$\n",
    "\n",
    "#### by taking the definition of our hypothesis $h_\\beta(x)$ we an equivalent expression,\n",
    "\n",
    "$ -\\Big(y \\ log \\ {1 \\over 1 + e^{-\\beta^Tx}} - (1-y) log(1 - {1 \\over 1 + e^{-\\beta^Tx}})\\Big)$\n",
    "\n",
    "- In the case where $y=1$ only the first term $y \\ log \\ {1 \\over 1 + e^{-\\beta^Tx}}$ matters.\n",
    "- The second term $(1-y) log(1 - {1 \\over 1 + e^{-\\beta^Tx}})$ becomes $0$, therefore we have $(1-1) log(1 - {1 \\over 1 + e^{-\\beta^Tx}})$\n",
    "\n",
    "Note that \n",
    "- When $y=1$ the value given to the cost function decreases as $\\beta^Tx$ becomes $\\gg 0$\n",
    "- When $y=0$ the value given to the cost function decreases as $\\beta^Tx$ becomes $\\ll 0$\n",
    "\n",
    "### Visual interpretation of last two statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def class_cost(x, example_class = 1):\n",
    "    '''call loss function for either positive training example or negative training example'''\n",
    "    a = []\n",
    "    for item in x:\n",
    "        if example_class == 1:\n",
    "            a.append(-(np.log(1/(1+(np.exp(-item))))))\n",
    "        elif example_class == 0:\n",
    "            a.append(-(np.log(1 - 1/(1+(np.exp(-item))))))\n",
    "    return a\n",
    "\n",
    "def plot_class_cost(x, example_class = 1):\n",
    "    '''plot output of cost function from a range of inputs given by x'''\n",
    "    cost = class_cost(x, example_class)\n",
    "    plt.plot(x, cost)\n",
    "    plt.title(\"Cost given from single training example y=%d\" %example_class, fontsize=20)\n",
    "    plt.xlabel(\"$z$\", fontsize=28)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------------------------\")\n",
    "print(\"values of cost derived from our hypothesis h_beta:\")\n",
    "print(\"--------------------------------\")\n",
    "print(class_cost(x = [-4.6, -3, -2, -1, 0, 1, 2, 3, 4.6], example_class=0))\n",
    "print(\"--------------------------------\")\n",
    "plot_class_cost(x, example_class=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, \n",
    "- when $y=1$, as our value of $z$ decreases the cost increases\n",
    "- when $y=0$, as our value of $z$ increases the cost increases\n",
    "\n",
    "**Essentially**, we are being penalized more and more as our prediction becomes more \"wrong\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic\n",
    "$- \\min\\limits_{\\beta} {1\\over m} \\sum \\limits_{i=1}^n \\Big(y_i \\ log \\ h_\\beta(x_i) + (1-y_i) log(1-h_\\beta(x_i))\\Big) + {\\lambda \\over 2m} \\sum \\limits_{i=1}^n \\beta_j^2$\n",
    "\n",
    "### SVM\n",
    "$- \\min\\limits_{\\beta} C \\sum \\limits_{i=1}^n \\Big(y_i \\ log \\ h_\\beta(x_i) + (1-y_i) log(1-h_\\beta(x_i))\\Big) + {1 \\over 2} \\sum \\limits_{i=1}^n \\beta_j^2$\n",
    "\n",
    "#### Regularization term C\n",
    "- C is a tradeoff between classifying points correctly and optimizing the margin of the decision boundary.\n",
    "- A large C will classify points in training set correctly at the expense of the margin. In practice, this leaves us more prone to overfitting. \n",
    "- A small C will maximize the margin at the expense of misclassifications. In practice, this leaves us more prone to underfitting.\n",
    "\n",
    "In other words\n",
    "- A large $C$ means less regularization.\n",
    "- A large $C$ makes the decision boundary more sensitive to noise, i.e. low bias, high variance.\n",
    "- A large $C$ is comparable to small $1 \\over \\lambda$ (or a large $\\lambda$).\n",
    "\n",
    "\n",
    "Once the parameters have been learned logistic regression output corresponds to a probability of belonging to class 1, SVM predicts class directly as,\n",
    "\n",
    "$h_\\beta(x) = 1, \\ if \\ \\beta^Tx \\geq 0, \\ otherwise \\ 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function  \n",
    "\n",
    "![](svm_cost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost(parameters, x, y = 1, C = 1):\n",
    "    theta_T_x = y*(parameters.transpose()*x) + (1-y)*(parameters.transpose()*x) ## cost function\n",
    "    z = C*(np.sum(theta_T_x) + 1/2*(np.sum(parameters)**2))    \n",
    "    \n",
    "    if y == 1:\n",
    "        if z >= 1:\n",
    "            cost = 0\n",
    "        else:\n",
    "            cost = -1*z + 1 ## if z = 0, then cost = 1\n",
    "    else:\n",
    "        if z <= -1:\n",
    "            cost = 0\n",
    "        else:\n",
    "            cost = 1*z + 1\n",
    "    return(theta_T_x, z, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost(parameters = np.array([.5, .5, 2]), x = np.array([1, 4, 4]), y=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want extra padding on our decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a linearly separable case\n",
    "\n",
    "i.e. the data can be divided perfectly by a linear equation, which produces a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not all code is my own**:\n",
    "I was introduced to much of these concepts by Rahul Dave (Prof. at Harvard). He used Jake VanderPlas's (Prof. at Northwestern) code in a workshop I attended at BDF, you can find below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60) ## produces 50 x 2 feature matrix, 50 x 1 response vector.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data can be separated by a straight line but indeed many straight lines will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of each line in the above figure will perfectly predict the class for each training example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none', color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the middle line has the largest margin between the closest training examples in either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's fit an SVC to choose the best line for the data\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([xi, yj])\n",
    "    return ax.contour(X, Y, P, colors='k',\n",
    "                      levels=[-1, 0, 1], alpha=0.5,\n",
    "                      linestyles=['--', '-', '--'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our solid line separating the data, it corresponds to the largest margin between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what happens if n becomes larger? How does it influence our line and the support vectors (those *training* examples touching our line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#Stolen from Jake's notebooks, above: https://github.com/jakevdp/ESAC-stats-2014\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
    "\n",
    "def plot_svc_decision_function(clf, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x = np.linspace(plt.xlim()[0], plt.xlim()[1], 30)\n",
    "    y = np.linspace(plt.ylim()[0], plt.ylim()[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    P = np.zeros_like(X)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i, j] = clf.decision_function([[xi, yj]])\n",
    "    return ax.contour(X, Y, P, colors='k',\n",
    "                      levels=[-1, 0, 1], alpha=0.5,\n",
    "                      linestyles=['--', '-', '--'])\n",
    "\n",
    "def plot_svm(N):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    clf = SVC(kernel='rbf')\n",
    "    clf.fit(X, y)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "    plt.xlim(-1, 4)\n",
    "    plt.ylim(-1, 6)\n",
    "    plot_svc_decision_function(clf, plt.gca())\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                s=200, facecolors='none')\n",
    "    \n",
    "interact(plot_svm, N=[10, 200], kernel='linear');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact\n",
    "def plot_svm(N, show_support_vectors=False):\n",
    "    ## Generate isotropic Gaussian blobs for clustering\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N] ## X for range(N), where N = interact update\n",
    "    y = y[:N] ## y for range(N), where N = interact update\n",
    "    clf = SVC(kernel='linear') ## linear kernel\n",
    "    clf.fit(X, y)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring') \n",
    "    plt.xlim(-1, 4)\n",
    "    plt.ylim(-1, 6)\n",
    "    plot_svc_decision_function(clf, plt.gca())\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                s=200, facecolors='none')\n",
    "    if show_support_vectors == True:\n",
    "        print(\"Support Vectors position along first dimension\")\n",
    "        print(clf.support_vectors_[:, 0])\n",
    "        print('\\n')\n",
    "        print(\"Support Vectors position along second dimension\")\n",
    "        print(clf.support_vectors_[:, 1])\n",
    "    \n",
    "    \n",
    "interact(plot_svm, N=[10, 200], kernel='linear');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about reality? Non-linear cases.\n",
    "\n",
    "In most cases you will deal with as a data scientist the data will not be linearly separable. So, how does an SVM do with non-linear decision functions? Well, it does quite well.\n",
    "\n",
    "Before we go completely non-linear, it's worth noting that a linear SVM will give very similar results to a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data is not separable by a linear equation. Note that much of the training examples denoted by the purple class are on the wrong side of the decision function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ what is called a kernel trick that maps our features to landmarks in the data. The landmarks are other training examples in the data set. Our features then become the landmarks and in most kernels the values of the feature vectors represent the distance of the $i^{th}$ training example to the $j^{th}$ feature vector, where the $j^{th}$ feature vector is a landmark representation of another training example.\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/75194/BDF/img1331.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see what using a Radial Gaussian kernel looks like...\n",
    "\n",
    "$$e^{-\\gamma d(x_1,x_2)^2} \\tag{1}$$\n",
    "---\n",
    "\n",
    "More commonly written as: $$K(x, x') = exp \\big(- \\frac{||x-x'||^2} {2\\sigma^2} \\big) \\tag{2}$$\n",
    "---\n",
    "The numerator represents the squared euclidean distance between $x$ and $x'$. In our first expression of the radial gaussian,\n",
    "\n",
    "$$\\gamma = \\frac{1}{2\\sigma^2} \\tag{3}$$\n",
    "---\n",
    "$$d(x,x') = ||x-x'||$$\n",
    "\n",
    "\n",
    "The $\\sigma$ is a free parameter that sets  the  width  of  the  bell-shaped  curve. \n",
    "- A \"large\" $\\gamma$ (small $\\sigma$) corresponds to a smoother kernel function thus low bias, high variance.\n",
    "- A \"small\" $\\gamma$ (large $\\sigma$) corresponds to a non-smooth kernel function thus high bias, low variance.\n",
    "\n",
    "\n",
    "In practice this parameter can be tuned through cross-validation. In `sklearn` learn this corresponds to parameter `gamma` of an `SVC` estimator and the default value is `1/n_features`. \n",
    "\n",
    "- w.r.t equation 3, a large `gamma` corresponds to a small $\\sigma$ while a small `gamma` corresponds to a large $\\sigma$ and vice versa.\n",
    "\n",
    "What does this mean for behavior of `gamma` w.r.t bias, variance tradeoff?\n",
    "\n",
    "- Small `gamma` means high bias, low varaince.\n",
    "- Large `gamma` means low bias, high variance.\n",
    "\n",
    "## Here is a visual illustration!\n",
    "-  `C` increases row-wise, `gamma` increases column-wise\n",
    " - Remember that increasing `C` means less regularization thus moving us from low to high variance.\n",
    "![gamma interacting with C](http://scikit-learn.org/stable/_images/sphx_glr_plot_rbf_parameters_001.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the gaussian kernel also called RBF (radian basis function) - a very common kernel in practice.\n",
    "r = np.exp(-(X[:, 0] ** 2 + X[:, 1] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='spring')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[-90, 90], azip=(-180, 180));\n",
    "\n",
    "## set to elev = 19, azim = 40 for clear snapshot of now separable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have our input space, below we have feature space as a function of the RBF. The data is now easily separable in our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(r, X[:, 1], c=y, s=50, cmap='spring');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data is now separable! Let's fit a SVC to the data with the rbf hyperparameter in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C = .4) ## use radial basis function for a non-linear hypothesis function\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='spring')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're a bit confused check out the next image. On the left we have our input space to the right we have our features space defined by a *polynomial* kernel $K(x, y)$. Remeber in this notebook we are using RBF.\n",
    "\n",
    "For more info see Wikipedia: https://en.wikipedia.org/wiki/Polynomial_kernel\n",
    "\n",
    "![polynomial feature mapping from input space to feature space](Svm_8_polynomial.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit learn implementation of SVM allows for 3 parameters to be tuned\n",
    "\n",
    "- `kernel`: can be 'rbf' (radial basis function) or 'linear', among others. This controls whether a linear or some other kernel is used to fit the data.\n",
    "- `C`: the SVC penalty parameter\n",
    "- `gamma`: the kernel coefficient for rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the hyperparameters of our non-linear model that we just fit? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## specify the grid of hyperparameters we want to try\n",
    "svc_param_grid = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 'auto'],\n",
    "                             'C': [1, 10, 100, 1000]},\n",
    "                            {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "clf = SVC() ## create instance of SVC\n",
    "clf = GridSearchCV(estimator=clf, param_grid = svc_param_grid, cv=5) ## use clf, specify param_grid, use 5 fold CV.\n",
    "clf.fit(X, y) ## fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## use method best_params_ to return the hyperparameters in our grid that minimized our cost function.\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I really like about scikit-learn is that is returns the results of CV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cross_validation_results = pd.DataFrame(clf.cv_results_) ## get results of CV by using method cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validation_results.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building scikit-learn pipelines to cv models and benchmark classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelcomp(X_train, y_train, X_test, y_test, algos=['Random Forest', 'AdaBoost', 'Logistic', 'SVM'], logistic_penalty='l2', cv=5, store_model_attr=True, AdaBoost_L2_pipeline=True):\n",
    "    \n",
    "    scoreDict = {}\n",
    "    \n",
    "    \n",
    "    if 'Random Forest' in algos:\n",
    "        ##--random forest--##\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "        rfc = RandomForestClassifier(n_estimators=10, criterion='gini', \n",
    "                                     max_features='auto', bootstrap=True, \n",
    "                                     oob_score=True)\n",
    "\n",
    "        rfc_param_grid = {\n",
    "        'n_estimators': [200, 400, 600],\n",
    "        'max_features' : ['auto', 'log2'],\n",
    "        'criterion' : ['gini', 'entropy']\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # fit model with CV\n",
    "        cv_rfc = GridSearchCV(estimator=rfc, param_grid=rfc_param_grid, cv=cv)\n",
    "        cv_rfc.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        scoreDict['Random Forest'] = cv_rfc.score(X_test, y_test)\n",
    "        \n",
    "        \n",
    "    if 'AdaBoost' in algos:\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        ##--AdaBoost--##\n",
    "        abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, \n",
    "                           algorithm='SAMME.R')\n",
    "\n",
    "        abc_param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate' : [0.4, 0.6, 0.8, 1.0],\n",
    "            'algorithm' : ['SAMME.R', 'SAMME']\n",
    "        }\n",
    "\n",
    "        cv_abc = GridSearchCV(estimator=abc, param_grid=abc_param_grid, cv=cv)\n",
    "        cv_abc.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # fit model with CV\n",
    "        cv_abs = GridSearchCV(estimator=abc, param_grid=abc_param_grid, cv=cv)\n",
    "        cv_abc.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "    \n",
    "        scoreDict['AdaBoost'] = cv_abc.score(X_test, y_test)\n",
    "\n",
    "\n",
    "    if 'Logistic Regression' in algos:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        ##--Logistic Regression--##\n",
    "        lr= LogisticRegression(penalty=logistic_penalty, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "        lr_param_grid = {\n",
    "            'C' : [1, 2, 3, 4, 5, 10, 20, 40]\n",
    "        }\n",
    "\n",
    "        cv_lr = GridSearchCV(estimator=lr, param_grid=lr_param_grid, cv=cv)\n",
    "\n",
    "        cv_lr.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # fit model with CV\n",
    "        cv_lr = GridSearchCV(estimator=lr, param_grid=lr_param_grid, cv=cv)\n",
    "        cv_lr.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "        scoreDict['Logistic Regression %s'%logistic_penalty] = cv_lr.score(X_test, y_test)\n",
    "\n",
    "    \n",
    "    if 'SVM' in algos: \n",
    "        from sklearn.svm import SVC\n",
    "        ##--Support Vector Machine/Classifier--##\n",
    "        svc = SVC(probability=True)\n",
    "\n",
    "        # Set the parameters by cross-validation\n",
    "        svc_param_grid = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 'auto'],\n",
    "                             'C': [1, 10, 100, 1000]},\n",
    "                            {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "        cv_svc = GridSearchCV(estimator=svc, param_grid=svc_param_grid, cv=5)\n",
    "        cv_svc.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # fit model with CV\n",
    "        cv_svc = GridSearchCV(estimator=svc, param_grid=svc_param_grid, cv=cv)\n",
    "        cv_svc.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        \n",
    "        scoreDict['SVM'] = cv_svc.score(X_test, y_test)\n",
    "        \n",
    "    # report best model - model with highest score in dict\n",
    "    return scoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreDict = modelcomp(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, algos=['Random Forest', \n",
    "                                                                                             'Logistic Regression', \n",
    "                                                                                             'SVM', 'AdaBoost'])\n",
    "\n",
    "scoreDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "Now we will apply the concept review and code written above on our own data set of choice. Please take 15 minutes to import any data set that you find interesting and fit a SVM using scikit-learn. I encourage you to;\n",
    "\n",
    "- optimize your hyperparameters using grid search and explore the CV results.\n",
    "- benchmark your SVM model against models produced with other classifiers (i.e. logistic regression, random forest, boosting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
